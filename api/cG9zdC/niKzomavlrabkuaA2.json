{"title":"网络爬虫学习-beautifulsoup的使用","date":"2018-12-21T16:00:00.000Z","slug":"爬虫学习6","comments":true,"updated":"2018-12-23T07:02:08.280Z","content":"<p>一、Beautiful Soup简介</p>\n<pre><code>简单来说，Beautiful Soup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下：\n</code></pre><p>Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。<br>Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。<br>Beautiful Soup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。</p>\n<pre><code>废话不多说，直接开始动手吧！\n</code></pre><p>二、实战</p>\n<p>1.背景介绍</p>\n<pre><code>小说网站-笔趣看： \nURL：http://www.biqukan.com/\n\n笔趣看是一个盗版小说网站，这里有很多起点中文网的小说，该网站小说的更新速度稍滞后于起点中文网正版小说的更新速度。并且该网站只支持在线浏览，不支持小说打包下载。因此，本次实战就是从该网站爬取并保存一本名为《一念永恒》的小说，该小说是耳根正在连载中的一部玄幻小说。PS：本实例仅为交流学习，支持耳根大大，请上起点中文网订阅。\n</code></pre><p>2.Beautiful Soup安装</p>\n<pre><code>我们我可以使用pip3或者easy_install来安装，在cmd命令窗口中的安装命令分别如下：\n</code></pre><p>a)pip3安装</p>\n<p>pip3 install beautifulsoup41</p>\n<p>b)easy_install安装</p>\n<p>easy_install beautifulsoup41</p>\n<p>3.预备知识</p>\n<pre><code>更为详细内容，可参考官方文档： \nURL：http://beautifulsoup.readthedocs.io/zh_CN/latest/\n</code></pre><p>a)创建Beautiful Soup对象</p>\n<p>from bs4 import BeautifulSoup</p>\n<p>#html为解析的页面获得html信息,为方便讲解，自己定义了一个html文件</p>\n<p>html = “””</p>\n<html>\n\n<head><meta name=\"generator\" content=\"Hexo 3.8.0\"><br><title>Jack_Cui</title><br></head><br><body><br><p class=\"title\" name=\"blog\"><b>My Blog</b></p><br><li><!--注释--></li><br><a href=\"http://blog.csdn.net/c406495762/article/details/58716886\" class=\"sister\" id=\"link1\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a><br><br><a href=\"http://blog.csdn.net/c406495762/article/details/59095864\" class=\"sister\" id=\"link2\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(二)：利用urllib.urlopen发送数据</a><br><br><a href=\"http://blog.csdn.net/c406495762/article/details/59488464\" class=\"sister\" id=\"link3\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(三)：urllib.error异常</a><br><br><script src=\"/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887\"></script><script>L2Dwidget.init({\"log\":false,\"pluginJsPath\":\"lib/\",\"pluginModelPath\":\"assets/\",\"pluginRootPath\":\"live2dw/\",\"tagMode\":false});</script></body><br></html><br>“””<br><br>#创建Beautiful Soup对象<br>soup = BeautifulSoup(html,’lxml’)123456789101112131415161718192021<br><br>    如果将上述的html的信息写入一个html文件，打开效果是这样的(&lt;!–注释–&gt;为注释内容，不会显示)：<br><br><br><br><br><br><br><br>    同样，我们还可以使用本地HTML文件来创建对象，代码如下：<br><br>soup = BeautifulSoup(open(test.html),’lxml’)1<br><br>    使用如下代码格式化输出：<br><br><br><br>print(soup.prettify())1<br><br><br><br><br><br><br><br>b)Beautiful Soup四大对象<br><br>    Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种:<br><br><br>Tag<br>NavigableString<br>BeautifulSoup<br>Comment<br><br><br>(1)Tag<br><br>    Tag通俗点讲就是HTML中的一个个标签，例如<br><br><title>Jack_Cui</title>1<br><br>    上面的title就是HTML标签，标签加入里面包括的内容就是Tag，下面我们来感受一下怎样用 Beautiful Soup 来方便地获取 Tags。<br><br>    下面每一段代码中注释部分即为运行结果：<br><br><br><br>print(soup.title)<br>#<title>Jack_Cui</title><br><br>print(soup.head)<br>#<head><meta name=\"generator\" content=\"Hexo 3.8.0\"> <title>Jack_Cui</title></head>\n\n<p>print(soup.a)</p>\n<p>#<a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a></p>\n<p>print(soup.p)</p>\n<p>#</p><p class=\"title\" name=\"blog\"><b>My Blog</b></p>1234567891011<p></p>\n<pre><code>我们可以利用 soup加标签名轻松地获取这些标签的内容，是不是感觉比正则表达式方便多了？不过有一点是，它查找的是在所有内容中的第一个符合要求的标签，如果要查询所有的标签，我们在后面进行介绍。\n\n我们也可验证一下这些对象的类型：\n</code></pre><p>print(type(soup.title))</p>\n<p>#<class 'bs4.element.tag'=\"\">12</class></p>\n<pre><code>对于Tag，有两个重要的属性：name和attrs\n</code></pre><p>name：</p>\n<p>print(soup.name)<br>print(soup.title.name)</p>\n<p>#[document]</p>\n<p>#title1234</p>\n<pre><code>soup 对象本身比较特殊，它的 name 即为 [document]，对于其他内部标签，输出的值便为标签本身的名称。\n</code></pre><p>attrs：</p>\n<p>print(soup.a.attrs)</p>\n<p>#{‘class’: [‘sister’], ‘href’: ‘<a href=\"http://blog.csdn.net/c406495762/article/details/58716886&#39;\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/c406495762/article/details/58716886&#39;</a>, ‘id’: ‘link1’}12</p>\n<pre><code>在这里，我们把 a 标签的所有属性打印输出了出来，得到的类型是一个字典。\n\n如果我们想要单独获取某个属性，可以这样，例如我们获取a标签的class叫什么，两个等价的方法如下：\n</code></pre><p>print(soup.a[‘class’])<br>print(soup.a.get(‘class’))</p>\n<p>#[‘sister’]</p>\n<p>#[‘sister’]1234</p>\n<p>(2)NavigableString</p>\n<pre><code>既然我们已经得到了标签的内容，那么问题来了，我们要想获取标签内部的文字怎么办呢？很简单，用 .string 即可，例如\n</code></pre><p>print(soup.title.string)</p>\n<p>#Jack_Cui12</p>\n<p>(3)BeautifulSoup</p>\n<pre><code>BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象，是一个特殊的 Tag，我们可以分别获取它的类型，名称，以及属性：\n</code></pre><p>print(type(soup.name))<br>print(soup.name)<br>print(soup.attrs)</p>\n<p>#<class 'str'=\"\"></class></p>\n<p>#[document]</p>\n<p>#{}123456</p>\n<p>(4)Comment</p>\n<pre><code>Comment对象是一个特殊类型的NavigableString对象，其实输出的内容仍然不包括注释符号，但是如果不好好处理它，可能会对我们的文本处理造成意想不到的麻烦。\n</code></pre><p>print(soup.li)<br>print(soup.li.string)<br>print(type(soup.li.string))</p>\n<p>#<li><!--注释--></li></p>\n<p>#注释</p>\n<p>#<class 'bs4.element.comment'=\"\">123456</class></p>\n<pre><code>li标签里的内容实际上是注释，但是如果我们利用 .string 来输出它的内容，我们发现它已经把注释符号去掉了，所以这可能会给我们带来不必要的麻烦。\n\n我们打印输出下它的类型，发现它是一个 Comment 类型，所以，我们在使用前最好做一下判断，判断代码如下：\n</code></pre><p>from bs4 import element</p>\n<p>if type(soup.li.string) == element.Comment:<br>     print(soup.li.string)1234</p>\n<pre><code>上面的代码中，我们首先判断了它的类型，是否为 Comment 类型，然后再进行其他操作，如打印输出。\n</code></pre><p>c)遍历文档数</p>\n<p>(1)直接子节点(不包含孙节点)</p>\n<p>contents：</p>\n<pre><code>tag的content属性可以将tag的子节点以列表的方式输出：\n</code></pre><p>print(soup.body.contents)</p>\n<p>#[‘\\n’, </p><p class=\"title\" name=\"blog\"><b>My Blog</b></p>, ‘\\n’, <li><!--注释--></li>, ‘\\n’, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a>, <br>, ‘\\n’, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59095864\" id=\"link2\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(二)：利#用urllib.urlopen发送数据</a>, <br>, ‘\\n’, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59488464\" id=\"link3\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(三)：urllib.error异常</a>, <br>, ‘\\n’]123<p></p>\n<pre><code>输出方式为列表，我们可以用列表索引来获取它的某一个元素：\n</code></pre><p>print(soup.body.contents[1])</p>\n<p></p><p class=\"title\" name=\"blog\"><b>My Blog</b></p>12<p></p>\n<p>children：</p>\n<pre><code>它返回的不是一个 list，不过我们可以通过遍历获取所有子节点，它是一个 list 生成器对象：\n</code></pre><p>for child in soup.body.children:<br>     print(child)12</p>\n<pre><code>结果如下图所示：\n</code></pre><p>(2)搜索文档树</p>\n<p>  find_all(name, attrs, recursive, text, limit, **kwargs)：</p>\n<pre><code>find_all() 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件。\n</code></pre><p>1) name参数：</p>\n<pre><code>name 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉。\n</code></pre><p>传递字符：</p>\n<pre><code>最简单的过滤器是字符串，在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的&lt;a&gt;标签：\n</code></pre><p>print(soup.find_all(‘a’))</p>\n<p>#[‘\\n’, </p><p class=\"title\" name=\"blog\"><b>My Blog</b></p>, ‘\\n’, <li><!--注释--></li>, ‘\\n’, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a>, <br>, ‘\\n’, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59095864\" id=\"link2\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(二)：利用urllib.urlopen发送数据</a>, <br>, ‘\\n’, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59488464\" id=\"link3\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(三)：urllib.error异常</a>, <br>, ‘\\n’]123<p></p>\n<p>传递正则表达式：</p>\n<pre><code>如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示&lt;body&gt;和&lt;b&gt;标签都应该被找到\n</code></pre><p>import re<br>for tag in soup.find_all(re.compile(“^b”)):<br>     print(tag.name)</p>\n<p>#body</p>\n<p>#b</p>\n<p>#br</p>\n<p>#br</p>\n<p>#br12345678</p>\n<p>传递列表：</p>\n<pre><code>如果传入列表参数，Beautiful Soup会将与列表中任一元素匹配的内容返回，下面代码找到文档中所有&lt;title&gt;标签和&lt;b&gt;标签：\n</code></pre><p>print(soup.find_all([‘title’,’b’]))</p>\n<p>#[<title>Jack_Cui</title>, <b>My Blog</b>]12</p>\n<p>传递True：</p>\n<pre><code>True 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点：\n</code></pre><p>for tag in soup.find_all(True):<br>     print(tag.name)12</p>\n<pre><code>运行结果：\n</code></pre><p>2)attrs参数</p>\n<pre><code>我们可以通过 find_all() 方法的 attrs 参数定义一个字典参数来搜索包含特殊属性的tag。\n</code></pre><p>print(soup.find_all(attrs={“class”:”title”}))</p>\n<p>#[</p><p class=\"title\" name=\"blog\"><b>My Blog</b></p>]12<p></p>\n<p>3)recursive参数</p>\n<pre><code>调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False。\n</code></pre><p>4)text参数</p>\n<pre><code>通过 text 参数可以搜搜文档中的字符串内容，与 name 参数的可选值一样, text 参数接受字符串 , 正则表达式 , 列表, True。\n</code></pre><p>print(soup.find_all(text=”Python3网络爬虫(三)：urllib.error异常”))</p>\n<p>#[‘Python3网络爬虫(三)：urllib.error异常’]12</p>\n<p>5)limit参数</p>\n<pre><code>find_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果。\n\n文档树中有3个tag符合搜索条件,但结果只返回了2个,因为我们限制了返回数量：\n</code></pre><p>print(soup.find_all(“a”, limit=2))</p>\n<p>#[<a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a>, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59095864\" id=\"link2\" target=\"_blank\" rel=\"noopener\">Python3网络爬虫(二)：利用urllib.urlopen发送数据</a>]123</p>\n<p>6)kwargs参数</p>\n<pre><code>如果传入 class 参数,Beautiful Soup 会搜索每个 class 属性为 title 的 tag 。kwargs 接收字符串，正则表达式\n</code></pre><p>print(soup.find_all(class_=”title”))</p>\n<p>#[</p><p class=\"title\" name=\"blog\"><b>My Blog</b></p>]12<p></p>\n<p>4.小说内容爬取</p>\n<pre><code>掌握以上内容就可以进行本次实战练习了\n</code></pre><p>a)单章小说内容爬取</p>\n<pre><code>打开《一念永恒》小说的第一章，进行审查元素分析。\n\nURL：http://www.biqukan.com/1_1094/5403177.html\n\n由审查结果可知，文章的内容存放在id为content，class为showtxt的div标签中：\n\n\n\n\n\n\n\n局部放大：\n\n\n\n\n\n\n\n因此我们，可以使用如下方法将本章小说内容爬取下来：\n</code></pre><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> -*- coding:UTF-8 -*-</span><br><span class=\"line\"></span><br><span class=\"line\">from urllib import request</span><br><span class=\"line\">from bs4 import BeautifulSoup</span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &quot;__main__&quot;:</span><br><span class=\"line\"> download_url = &apos;http://www.biqukan.com/1_1094/5403177.html&apos;</span><br><span class=\"line\"> head = &#123;&#125;</span><br><span class=\"line\"> head[&apos;User-Agent&apos;] = &apos;Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19&apos;</span><br><span class=\"line\"> download_req = request.Request(url = download_url, headers = head)</span><br><span class=\"line\"> download_response = request.urlopen(download_req)</span><br><span class=\"line\"> download_html = download_response.read().decode(&apos;gbk&apos;,&apos;ignore&apos;)</span><br><span class=\"line\"> soup_texts = BeautifulSoup(download_html, &apos;lxml&apos;)</span><br><span class=\"line\"> texts = soup_texts.find_all(id = &apos;content&apos;, class_ = &apos;showtxt&apos;)</span><br><span class=\"line\"> soup_text = BeautifulSoup(str(texts), &apos;lxml&apos;)</span><br></pre></td></tr></table></figure>\n<p>from urllib import request<br>from bs4 import BeautifulSoup</p>\n<p>if <strong>name</strong> == “<strong>main</strong>“:<br>    download_url = ‘<a href=\"http://www.biqukan.com/1_1094/5403177.html&#39;\" target=\"_blank\" rel=\"noopener\">http://www.biqukan.com/1_1094/5403177.html&#39;</a><br>    head = {}<br>    head[‘User-Agent’] = ‘Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19’<br>    download_req = request.Request(url = download_url, headers = head)<br>    download_response = request.urlopen(download_req)<br>    download_html = download_response.read().decode(‘gbk’,’ignore’)<br>    soup_texts = BeautifulSoup(download_html, ‘lxml’)<br>    texts = soup_texts.find_all(id = ‘content’, class_ = ‘showtxt’)<br>    soup_text = BeautifulSoup(str(texts), ‘lxml’)</p>\n<pre><code>#将\\xa0无法解码的字符删除\nprint(soup_text.div.text.replace(&apos;\\xa0&apos;,&apos;&apos;))12345678910111213141516\n\n运行结果：\n\n\n\n\n\n\n\n可以看到，我们已经顺利爬取第一章内容，接下来就是如何爬取所有章的内容，爬取之前需要知道每个章节的地址。因此，我们需要审查《一念永恒》小说目录页的内容。\n</code></pre><p>b)各章小说链接爬取</p>\n<pre><code>URL：http://www.biqukan.com/1_1094/\n\n由审查结果可知，小说每章的链接放在了class为listmain的div标签中。链接具体位置放在html-&gt;body-&gt;div-&gt;dd-&gt;dl-&gt;a的href属性中，例如下图的第759章的href属性为/1_1094/14235101.html，那么该章节的地址为：http://www.biqukan.com/1_1094/14235101.html\n\n\n\n\n\n\n\n局部放大：\n\n\n\n\n\n\n\n因此，我们可以使用如下方法获取正文所有章节的地址：\n</code></pre><h1 id=\"coding-UTF-8\"><a href=\"#coding-UTF-8\" class=\"headerlink\" title=\"-- coding:UTF-8 --\"></a>-<em>- coding:UTF-8 -</em>-</h1><p>from urllib import request<br>from bs4 import BeautifulSoup</p>\n<p>if <strong>name</strong> == “<strong>main</strong>“:<br>    target_url = ‘<a href=\"http://www.biqukan.com/1_1094/&#39;\" target=\"_blank\" rel=\"noopener\">http://www.biqukan.com/1_1094/&#39;</a><br>    head = {}<br>    head[‘User-Agent’] = ‘Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19’<br>    target_req = request.Request(url = target_url, headers = head)<br>    target_response = request.urlopen(target_req)<br>    target_html = target_response.read().decode(‘gbk’,’ignore’)</p>\n<pre><code>#创建BeautifulSoup对象\nlistmain_soup = BeautifulSoup(target_html,&apos;lxml&apos;)\n#搜索文档树,找出div标签中class为listmain的所有子标签\nchapters = listmain_soup.find_all(&apos;div&apos;,class_ = &apos;listmain&apos;)\n#使用查询结果再创建一个BeautifulSoup对象,对其继续进行解析\ndownload_soup = BeautifulSoup(str(chapters), &apos;lxml&apos;)\n#开始记录内容标志位,只要正文卷下面的链接,最新章节列表链接剔除\nbegin_flag = False\n#遍历dl标签下所有子节点\nfor child in download_soup.dl.children:\n    #滤除回车\n    if child != &apos;\\n&apos;:\n        #找到《一念永恒》正文卷,使能标志位\n        if child.string == u&quot;《一念永恒》正文卷&quot;:\n            begin_flag = True\n        #爬取链接\n        if begin_flag == True and child.a != None:\n            download_url = &quot;http://www.biqukan.com&quot; + child.a.get(&apos;href&apos;)\n            download_name = child.string\n            print(download_name + &quot; : &quot; + download_url)12345678910111213141516171819202122232425262728293031\n\n运行结果：\n</code></pre><p>c)爬取所有章节内容，并保存到文件中</p>\n<pre><code>整合以上代码，并进行相应处理，编写如下代码：\n</code></pre><h1 id=\"coding-UTF-8-1\"><a href=\"#coding-UTF-8-1\" class=\"headerlink\" title=\"-- coding:UTF-8 --\"></a>-<em>- coding:UTF-8 -</em>-</h1><p>from urllib import request<br>from bs4 import BeautifulSoup<br>import re<br>import sys</p>\n<p>if <strong>name</strong> == “<strong>main</strong>“:</p>\n<pre><code>#创建txt文件\nfile = open(&apos;一念永恒.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;)\n#一念永恒小说目录地址\ntarget_url = &apos;http://www.biqukan.com/1_1094/&apos;\n#User-Agent\nhead = {}\nhead[&apos;User-Agent&apos;] = &apos;Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19&apos;\ntarget_req = request.Request(url = target_url, headers = head)\ntarget_response = request.urlopen(target_req)\ntarget_html = target_response.read().decode(&apos;gbk&apos;,&apos;ignore&apos;)\n#创建BeautifulSoup对象\nlistmain_soup = BeautifulSoup(target_html,&apos;lxml&apos;)\n#搜索文档树,找出div标签中class为listmain的所有子标签\nchapters = listmain_soup.find_all(&apos;div&apos;,class_ = &apos;listmain&apos;)\n#使用查询结果再创建一个BeautifulSoup对象,对其继续进行解析\ndownload_soup = BeautifulSoup(str(chapters), &apos;lxml&apos;)\n#计算章节个数\nnumbers = (len(download_soup.dl.contents) - 1) / 2 - 8\nindex = 1\n#开始记录内容标志位,只要正文卷下面的链接,最新章节列表链接剔除\nbegin_flag = False\n#遍历dl标签下所有子节点\nfor child in download_soup.dl.children:\n    #滤除回车\n    if child != &apos;\\n&apos;:\n        #找到《一念永恒》正文卷,使能标志位\n        if child.string == u&quot;《一念永恒》正文卷&quot;:\n            begin_flag = True\n        #爬取链接并下载链接内容\n        if begin_flag == True and child.a != None:\n            download_url = &quot;http://www.biqukan.com&quot; + child.a.get(&apos;href&apos;)\n            download_req = request.Request(url = download_url, headers = head)\n            download_response = request.urlopen(download_req)\n            download_html = download_response.read().decode(&apos;gbk&apos;,&apos;ignore&apos;)\n            download_name = child.string\n            soup_texts = BeautifulSoup(download_html, &apos;lxml&apos;)\n            texts = soup_texts.find_all(id = &apos;content&apos;, class_ = &apos;showtxt&apos;)\n            soup_text = BeautifulSoup(str(texts), &apos;lxml&apos;)\n            write_flag = True\n            file.write(download_name + &apos;\\n\\n&apos;)\n            #将爬取内容写入文件\n            for each in soup_text.div.text.replace(&apos;\\xa0&apos;,&apos;&apos;):\n                if each == &apos;h&apos;:\n                    write_flag = False\n                if write_flag == True and each != &apos; &apos;:\n                    file.write(each)\n                if write_flag == True and each == &apos;\\r&apos;:\n                    file.write(&apos;\\n&apos;)\n            file.write(&apos;\\n\\n&apos;)\n            #打印爬取进度\n            sys.stdout.write(&quot;已下载:%.3f%%&quot; % float(index/numbers) + &apos;\\r&apos;)\n            sys.stdout.flush()\n            index += 1\nfile.close()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061\n\n代码略显粗糙，运行效率不高，还有很多可以改进的地方，运行效果如下图所示：\n\n\n\n\n\n\n\n最终生成的txt文件，如下图所示：\n\n\n\n\n\n\n\n生成的txt文件，可以直接拷贝到手机中进行阅读，手机阅读软件可以解析这样排版的txt文件。\n\nPS：如果觉得本篇本章对您有所帮助，欢迎关注、评论、点赞，谢谢！\n\n参考文章： \nURL：http://cuiqingcai.com/1319.html\n</code></pre>","prev":{"title":"HTTP响应状态码参考：","slug":"pxcc"},"next":{"title":"网络爬虫学习1","slug":"IP代理的使用"},"link":"http://www.blog.oyxwxn.com/post/爬虫学习6/","toc":[{"title":"-<em>- coding:UTF-8 -</em>-","id":"coding-UTF-8","index":"1"},{"title":"-<em>- coding:UTF-8 -</em>-","id":"coding-UTF-8-1","index":"2"}],"copyright":{"author":"ufo","published":"December 22, 2018","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"},"reward":true}